"""
ìºì‹œ ê´€ë¦¬ ëª¨ë“ˆ (ì„±ëŠ¥ ìµœì í™” ë²„ì „)
ì‹¤ì‹œê°„ ë°ì´í„° ë°˜ì˜ê³¼ ê³ ì„±ëŠ¥ ìºì‹±ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import os
import sys
import time
import threading
import hashlib
import json
import gzip
import pickle
from typing import Any, Optional, Dict, List, Callable, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import OrderedDict
import pytz
try:
    import psutil
except ImportError:
    psutil = None

# ê²½ë¡œ ì„¤ì • (VM í™˜ê²½ ëŒ€ì‘)
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(os.path.dirname(os.path.abspath(__file__)), '..'))

try:
    from config.settings import config
    from utils.logging_config import logger
except ImportError:
    # VM í™˜ê²½ì—ì„œ ì„í¬íŠ¸ ì‹¤íŒ¨ ì‹œ í´ë°±
    import logging
    logger = logging.getLogger('cache_manager')
    
    # ê¸°ë³¸ ì„¤ì •ê°’
    class FallbackConfig:
        DEBUG_MODE = False
        FORTUNE_CACHE_TTL = 3600
        MAX_CACHE_SIZE = 1000
        COMPRESSION_THRESHOLD = 1024  # 1KB ì´ìƒ ì••ì¶•
    config = FallbackConfig()

# KST íƒ€ì„ì¡´
KST = pytz.timezone('Asia/Seoul')


@dataclass
class CacheItem:
    """ìºì‹œ ì•„ì´í…œ (ì„±ëŠ¥ ìµœì í™”)"""
    key: str
    value: Any
    created_at: datetime
    accessed_at: datetime
    access_count: int = 0
    size_bytes: int = 0
    compressed: bool = False
    
    def update_access(self) -> None:
        """ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸"""
        self.accessed_at = datetime.now()
        self.access_count += 1
    
    def get_age_seconds(self) -> float:
        """ìƒì„± í›„ ê²½ê³¼ ì‹œê°„ (ì´ˆ)"""
        return (datetime.now() - self.created_at).total_seconds()
    
    def get_idle_seconds(self) -> float:
        """ë§ˆì§€ë§‰ ì ‘ê·¼ í›„ ê²½ê³¼ ì‹œê°„ (ì´ˆ)"""
        return (datetime.now() - self.accessed_at).total_seconds()


@dataclass
class DailyFortuneCache:
    """ìœ ì €ë³„ ì¼ì¼ ìš´ì„¸ ìºì‹œ ì•„ì´í…œ (ìµœì í™”)"""
    user_id: str
    fortune_text: str
    date: str  # YYYY-MM-DD í˜•ì‹
    created_at: datetime
    access_count: int = 0
    
    def is_valid_for_date(self, target_date: str) -> bool:
        """ì§€ì •ëœ ë‚ ì§œì— ëŒ€í•´ ìœ íš¨í•œì§€ í™•ì¸"""
        return self.date == target_date
    
    def update_access(self) -> None:
        """ì ‘ê·¼ íšŸìˆ˜ ì—…ë°ì´íŠ¸"""
        self.access_count += 1


@dataclass
class ContentCache:
    """ì½˜í…ì¸  ìºì‹œ ì•„ì´í…œ (ìµœì í™”)"""
    content_type: str
    data: Any
    created_at: datetime
    ttl: int  # TTL (ì´ˆ)
    access_count: int = 0
    size_bytes: int = 0
    compressed: bool = False
    
    def is_expired(self) -> bool:
        """ìºì‹œ ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        return (datetime.now() - self.created_at).total_seconds() > self.ttl
    
    def update_access(self) -> None:
        """ì ‘ê·¼ íšŸìˆ˜ ì—…ë°ì´íŠ¸"""
        self.access_count += 1


@dataclass
class CacheMetrics:
    """ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    total_requests: int = 0
    total_hits: int = 0
    total_misses: int = 0
    total_evictions: int = 0
    total_compressions: int = 0
    total_decompressions: int = 0
    memory_usage_bytes: int = 0
    compression_ratio: float = 1.0
    average_response_time: float = 0.0
    last_cleanup_time: float = 0.0


class LRUCache:
    """LRU (Least Recently Used) ìºì‹œ êµ¬í˜„"""
    
    def __init__(self, max_size: int = 1000):
        self.max_size = max_size
        self.cache: OrderedDict = OrderedDict()
        self._lock = threading.RLock()
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        with self._lock:
            if key in self.cache:
                # LRU ìˆœì„œ ì—…ë°ì´íŠ¸
                value = self.cache.pop(key)
                self.cache[key] = value
                return value
            return None
    
    def put(self, key: str, value: Any) -> None:
        """ìºì‹œì— ê°’ ì €ì¥"""
        with self._lock:
            if key in self.cache:
                # ê¸°ì¡´ ê°’ ì œê±°
                self.cache.pop(key)
            elif len(self.cache) >= self.max_size:
                # LRU í•­ëª© ì œê±°
                self.cache.popitem(last=False)
            
            self.cache[key] = value
    
    def remove(self, key: str) -> bool:
        """ìºì‹œì—ì„œ í•­ëª© ì œê±°"""
        with self._lock:
            if key in self.cache:
                self.cache.pop(key)
                return True
            return False
    
    def clear(self) -> None:
        """ìºì‹œ ì „ì²´ ë¹„ìš°ê¸°"""
        with self._lock:
            self.cache.clear()
    
    def size(self) -> int:
        """ìºì‹œ í¬ê¸° ë°˜í™˜"""
        return len(self.cache)
    
    def keys(self) -> List[str]:
        """ìºì‹œ í‚¤ ëª©ë¡ ë°˜í™˜"""
        return list(self.cache.keys())


class CacheCompressor:
    """ìºì‹œ ì••ì¶•/í•´ì œ ìœ í‹¸ë¦¬í‹°"""
    
    @staticmethod
    def compress_data(data: Any) -> Tuple[bytes, bool]:
        """ë°ì´í„° ì••ì¶•"""
        try:
            # ë°ì´í„°ë¥¼ pickleë¡œ ì§ë ¬í™”
            serialized = pickle.dumps(data)
            
            # ì••ì¶• ì„ê³„ê°’ í™•ì¸
            if len(serialized) < getattr(config, 'COMPRESSION_THRESHOLD', 1024):
                return serialized, False
            
            # gzip ì••ì¶•
            compressed = gzip.compress(serialized)
            
            # ì••ì¶•ë¥  í™•ì¸ (ì••ì¶•ì´ íš¨ê³¼ì ì´ë©´ ì••ì¶•ëœ ë°ì´í„° ë°˜í™˜)
            if len(compressed) < len(serialized):
                return compressed, True
            else:
                return serialized, False
                
        except Exception as e:
            logger.warning(f"ë°ì´í„° ì••ì¶• ì‹¤íŒ¨: {e}")
            return pickle.dumps(data), False
    
    @staticmethod
    def decompress_data(compressed_data: bytes, is_compressed: bool) -> Any:
        """ë°ì´í„° í•´ì œ"""
        try:
            if is_compressed:
                # gzip í•´ì œ
                decompressed = gzip.decompress(compressed_data)
            else:
                decompressed = compressed_data
            
            # pickle ì—­ì§ë ¬í™”
            return pickle.loads(decompressed)
            
        except Exception as e:
            logger.error(f"ë°ì´í„° í•´ì œ ì‹¤íŒ¨: {e}")
            return None


class CacheManager:
    """ê³ ì„±ëŠ¥ ìºì‹œ ê´€ë¦¬ì (ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self):
        """CacheManager ì´ˆê¸°í™”"""
        self._fortune_cache: Dict[str, DailyFortuneCache] = {}
        self._content_cache: Dict[str, ContentCache] = {}
        self._lru_cache = LRUCache(max_size=getattr(config, 'MAX_CACHE_SIZE', 1000))
        self._compressor = CacheCompressor()
        self._lock = threading.RLock()
        self._metrics = CacheMetrics()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self._last_memory_check = time.time()
        self._memory_check_interval = 300  # 5ë¶„ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì²´í¬
        
        logger.info("ê³ ì„±ëŠ¥ ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_user_daily_fortune(self, user_id: str, fortune_text: str = None) -> Optional[str]:
        """
        ìœ ì €ë³„ ì¼ì¼ ìš´ì„¸ ì¡°íšŒ/ì €ì¥ (ì„±ëŠ¥ ìµœì í™”)
        
        Args:
            user_id: ì‚¬ìš©ì ID
            fortune_text: ìƒˆë¡œìš´ ìš´ì„¸ (ì €ì¥ìš©, Noneì´ë©´ ì¡°íšŒë§Œ)
            
        Returns:
            Optional[str]: ì €ì¥ëœ ìš´ì„¸ ë˜ëŠ” None
        """
        start_time = time.time()
        current_date = self._get_kst_date_string()
        cache_key = f"{user_id}:{current_date}"
        
        with self._lock:
            self._metrics.total_requests += 1
            
            # ì¡°íšŒ
            if cache_key in self._fortune_cache:
                cached_fortune = self._fortune_cache[cache_key]
                if cached_fortune.is_valid_for_date(current_date):
                    cached_fortune.update_access()
                    self._metrics.total_hits += 1
                    self._update_response_time(time.time() - start_time)
                    logger.debug(f"ìš´ì„¸ ìºì‹œ íˆíŠ¸: {user_id} ({current_date})")
                    return cached_fortune.fortune_text
                else:
                    # ë‚ ì§œê°€ ë°”ë€ ê²½ìš° ì‚­ì œ
                    del self._fortune_cache[cache_key]
            
            # ìƒˆë¡œìš´ ìš´ì„¸ ì €ì¥
            if fortune_text:
                self._fortune_cache[cache_key] = DailyFortuneCache(
                    user_id=user_id,
                    fortune_text=fortune_text,
                    date=current_date,
                    created_at=datetime.now()
                )
                self._metrics.total_misses += 1
                self._update_response_time(time.time() - start_time)
                logger.debug(f"ìš´ì„¸ ìºì‹œ ì €ì¥: {user_id} ({current_date})")
                return fortune_text
            
            # ì¡°íšŒë§Œ í•˜ëŠ” ê²½ìš° None ë°˜í™˜
            self._metrics.total_misses += 1
            self._update_response_time(time.time() - start_time)
            return None
    
    def get_content_with_cache(self, content_type: str, fetch_func: Callable) -> Any:
        """
        ì½˜í…ì¸ ë¥¼ ìºì‹œë¡œ ì¡°íšŒ (ì„±ëŠ¥ ìµœì í™”)
        
        Args:
            content_type: ì½˜í…ì¸  íƒ€ì…
            fetch_func: ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
            
        Returns:
            Any: ìºì‹œëœ ë°ì´í„° ë˜ëŠ” ìƒˆë¡œ ê°€ì ¸ì˜¨ ë°ì´í„°
        """
        start_time = time.time()
        
        with self._lock:
            self._metrics.total_requests += 1
            
            # ìºì‹œ í™•ì¸
            if content_type in self._content_cache:
                cached_content = self._content_cache[content_type]
                if not cached_content.is_expired():
                    cached_content.update_access()
                    self._metrics.total_hits += 1
                    self._update_response_time(time.time() - start_time)
                    logger.debug(f"ì½˜í…ì¸  ìºì‹œ íˆíŠ¸: {content_type}")
                    
                    # ì••ì¶•ëœ ë°ì´í„° í•´ì œ
                    if cached_content.compressed:
                        self._metrics.total_decompressions += 1
                        return self._compressor.decompress_data(
                            cached_content.data, True
                        )
                    return cached_content.data
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
                    del self._content_cache[content_type]
            
            # ìƒˆ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            try:
                data = fetch_func()
                
                # ë°ì´í„° ì••ì¶• ì‹œë„
                compressed_data, is_compressed = self._compressor.compress_data(data)
                
                # ìºì‹œì— ì €ì¥
                self._content_cache[content_type] = ContentCache(
                    content_type=content_type,
                    data=compressed_data,
                    created_at=datetime.now(),
                    ttl=3600,  # 1ì‹œê°„
                    size_bytes=len(compressed_data),
                    compressed=is_compressed
                )
                
                if is_compressed:
                    self._metrics.total_compressions += 1
                
                self._metrics.total_misses += 1
                self._update_response_time(time.time() - start_time)
                logger.debug(f"ì½˜í…ì¸  ìºì‹œ ì €ì¥: {content_type}")
                return data
                
            except Exception as e:
                logger.error(f"ì½˜í…ì¸  ì¡°íšŒ ì‹¤íŒ¨: {content_type} - {e}")
                self._metrics.total_misses += 1
                self._update_response_time(time.time() - start_time)
                return None
    
    def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """
        ì¼ë°˜ ìºì‹œì— ê°’ ì €ì¥ (LRU ì ìš©)
        
        Args:
            key: ìºì‹œ í‚¤
            value: ì €ì¥í•  ê°’
            ttl: TTL (ì´ˆ)
        """
        start_time = time.time()
        
        with self._lock:
            # ë°ì´í„° ì••ì¶•
            compressed_data, is_compressed = self._compressor.compress_data(value)
            
            # LRU ìºì‹œì— ì €ì¥
            self._lru_cache.put(key, {
                'data': compressed_data,
                'compressed': is_compressed,
                'created_at': datetime.now(),
                'ttl': ttl
            })
            
            if is_compressed:
                self._metrics.total_compressions += 1
            
            self._metrics.total_requests += 1
            self._update_response_time(time.time() - start_time)
    
    def get(self, key: str) -> Optional[Any]:
        """
        ì¼ë°˜ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ (LRU ì ìš©)
        
        Args:
            key: ìºì‹œ í‚¤
            
        Returns:
            Optional[Any]: ìºì‹œëœ ê°’ ë˜ëŠ” None
        """
        start_time = time.time()
        
        with self._lock:
            cached_item = self._lru_cache.get(key)
            
            if cached_item:
                # TTL í™•ì¸
                if (datetime.now() - cached_item['created_at']).total_seconds() > cached_item['ttl']:
                    self._lru_cache.remove(key)
                    self._metrics.total_misses += 1
                    self._update_response_time(time.time() - start_time)
                    return None
                
                # ì••ì¶•ëœ ë°ì´í„° í•´ì œ
                if cached_item['compressed']:
                    self._metrics.total_decompressions += 1
                    data = self._compressor.decompress_data(
                        cached_item['data'], True
                    )
                else:
                    data = self._compressor.decompress_data(
                        cached_item['data'], False
                    )
                
                self._metrics.total_hits += 1
                self._update_response_time(time.time() - start_time)
                return data
            else:
                self._metrics.total_misses += 1
                self._update_response_time(time.time() - start_time)
                return None
    
    def delete(self, key: str) -> bool:
        """
        ìºì‹œì—ì„œ í•­ëª© ì‚­ì œ
        
        Args:
            key: ì‚­ì œí•  í‚¤
            
        Returns:
            bool: ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        """
        with self._lock:
            return self._lru_cache.remove(key)
    
    def clear_old_entries(self) -> int:
        """
        ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì •ë¦¬
        
        Returns:
            int: ì •ë¦¬ëœ í•­ëª© ìˆ˜
        """
        with self._lock:
            removed_count = 0
            
            # ìš´ì„¸ ìºì‹œ ì •ë¦¬
            current_date = self._get_kst_date_string()
            keys_to_remove = []
            for key, cached_fortune in self._fortune_cache.items():
                if not cached_fortune.is_valid_for_date(current_date):
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                del self._fortune_cache[key]
                removed_count += 1
            
            # ì½˜í…ì¸  ìºì‹œ ì •ë¦¬
            keys_to_remove = []
            for key, cached_content in self._content_cache.items():
                if cached_content.is_expired():
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                del self._content_cache[key]
                removed_count += 1
            
            # LRU ìºì‹œ ì •ë¦¬
            keys_to_remove = []
            for key in self._lru_cache.keys():
                cached_item = self._lru_cache.get(key)
                if cached_item and (datetime.now() - cached_item['created_at']).total_seconds() > cached_item['ttl']:
                    keys_to_remove.append(key)
            
            for key in keys_to_remove:
                self._lru_cache.remove(key)
                removed_count += 1
            
            self._metrics.last_cleanup_time = time.time()
            return removed_count
    
    def optimize_memory_usage(self) -> Dict[str, Any]:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™”
        
        Returns:
            Dict[str, Any]: ìµœì í™” ê²°ê³¼
        """
        current_time = time.time()
        if current_time - self._last_memory_check < self._memory_check_interval:
            return {'status': 'skipped', 'reason': 'too_frequent'}
        
        self._last_memory_check = current_time
        
        if psutil:
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                process = psutil.Process()
                memory_info = process.memory_info()
                current_memory = memory_info.rss
                
                optimization_result = {
                    'status': 'optimized',
                    'before_memory_mb': current_memory / (1024 * 1024),
                    'actions_taken': []
                }
                
                with self._lock:
                    # 1. ì˜¤ë˜ëœ í•­ëª© ì •ë¦¬
                    removed_count = self.clear_old_entries()
                    if removed_count > 0:
                        optimization_result['actions_taken'].append(f'removed_old_entries: {removed_count}')
                    
                    # 2. LRU ìºì‹œ í¬ê¸° ì¡°ì •
                    if self._lru_cache.size() > self._lru_cache.max_size * 0.8:
                        # 20% ì œê±°
                        remove_count = int(self._lru_cache.size() * 0.2)
                        keys_to_remove = list(self._lru_cache.keys())[:remove_count]
                        for key in keys_to_remove:
                            self._lru_cache.remove(key)
                        optimization_result['actions_taken'].append(f'reduced_lru_cache: {remove_count}')
                    
                    # 3. ì••ì¶•ë¥ ì´ ë‚®ì€ í•­ëª© ì¬ì••ì¶•
                    recompressed_count = 0
                    for key, cached_content in self._content_cache.items():
                        if not cached_content.compressed and cached_content.size_bytes > 1024:
                            # ì¬ì••ì¶• ì‹œë„
                            compressed_data, is_compressed = self._compressor.compress_data(
                                self._compressor.decompress_data(cached_content.data, False)
                            )
                            if is_compressed and len(compressed_data) < cached_content.size_bytes:
                                cached_content.data = compressed_data
                                cached_content.compressed = True
                                cached_content.size_bytes = len(compressed_data)
                                recompressed_count += 1
                    
                    if recompressed_count > 0:
                        optimization_result['actions_taken'].append(f'recompressed_items: {recompressed_count}')
                
                # ìµœì í™” í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
                memory_info = process.memory_info()
                after_memory = memory_info.rss
                optimization_result['after_memory_mb'] = after_memory / (1024 * 1024)
                optimization_result['memory_saved_mb'] = (current_memory - after_memory) / (1024 * 1024)
                
                return optimization_result
                
            except Exception as e:
                logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨: {e}")
                return {'status': 'error', 'error': str(e)}
        else:
            return {'status': 'skipped', 'reason': 'psutil_not_available'}
    
    def get_detailed_stats(self) -> Dict[str, Any]:
        """
        ìƒì„¸í•œ ìºì‹œ í†µê³„ ë°˜í™˜
        
        Returns:
            Dict[str, Any]: ìƒì„¸í•œ í†µê³„ ì •ë³´
        """
        with self._lock:
            total_requests = self._metrics.total_requests
            hit_rate = (self._metrics.total_hits / total_requests * 100) if total_requests > 0 else 0
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
            memory_usage = 0
            for cached_content in self._content_cache.values():
                memory_usage += cached_content.size_bytes
            
            # ì••ì¶•ë¥  ê³„ì‚°
            compression_ratio = self._metrics.compression_ratio
            
            return {
                'fortune_cache_size': len(self._fortune_cache),
                'content_cache_size': len(self._content_cache),
                'lru_cache_size': self._lru_cache.size(),
                'total_requests': total_requests,
                'total_hits': self._metrics.total_hits,
                'total_misses': self._metrics.total_misses,
                'hit_rate': round(hit_rate, 2),
                'total_evictions': self._metrics.total_evictions,
                'total_compressions': self._metrics.total_compressions,
                'total_decompressions': self._metrics.total_decompressions,
                'memory_usage_mb': round(memory_usage / (1024 * 1024), 2),
                'compression_ratio': round(compression_ratio, 2),
                'average_response_time_ms': round(self._metrics.average_response_time * 1000, 2),
                'last_cleanup_time': self._metrics.last_cleanup_time
            }
    
    def _update_response_time(self, response_time: float) -> None:
        """ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        total_requests = self._metrics.total_requests
        current_avg = self._metrics.average_response_time
        new_avg = (current_avg * (total_requests - 1) + response_time) / total_requests
        self._metrics.average_response_time = new_avg
    
    def _get_kst_date_string(self) -> str:
        """í˜„ì¬ KST ë‚ ì§œë¥¼ YYYY-MM-DD í˜•ì‹ìœ¼ë¡œ ë°˜í™˜"""
        kst_now = datetime.now(KST)
        return kst_now.strftime('%Y-%m-%d')


class BotCacheManager:
    """ë´‡ ì „ìš© ìºì‹œ ê´€ë¦¬ì (ì„±ëŠ¥ ìµœì í™”)"""
    
    def __init__(self):
        """BotCacheManager ì´ˆê¸°í™”"""
        self.cache = CacheManager()
        # general_cache ì†ì„± ì¶”ê°€ (í•˜ìœ„ í˜¸í™˜ì„±)
        self.general_cache = self.cache
        logger.info("ê³ ì„±ëŠ¥ ë´‡ ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
    
    def get_user_daily_fortune(self, user_id: str, fortune_text: str = None) -> Optional[str]:
        """ìœ ì €ë³„ ì¼ì¼ ìš´ì„¸ ì¡°íšŒ/ì €ì¥"""
        return self.cache.get_user_daily_fortune(user_id, fortune_text)
    
    def get_fortune_phrases(self, fetch_func: Callable) -> List[str]:
        """ìš´ì„¸ ë¬¸êµ¬ ëª©ë¡ ì¡°íšŒ (1ì‹œê°„ ìºì‹œ)"""
        return self.cache.get_content_with_cache('fortune_list', fetch_func) or []
    
    def get_help_items(self, fetch_func: Callable) -> List[Dict]:
        """ë„ì›€ë§ í•­ëª© ì¡°íšŒ (1ì‹œê°„ ìºì‹œ)"""
        return self.cache.get_content_with_cache('help_items', fetch_func) or []
    
    def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """ì¼ë°˜ ìºì‹œì— ê°’ ì €ì¥"""
        self.cache.set(key, value, ttl)
    
    def get(self, key: str) -> Optional[Any]:
        """ì¼ë°˜ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        return self.cache.get(key)
    
    def delete(self, key: str) -> bool:
        """ìºì‹œì—ì„œ í•­ëª© ì‚­ì œ"""
        return self.cache.delete(key)
    
    def clear_old_entries(self) -> int:
        """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
        return self.cache.clear_old_entries()
    
    def optimize_memory(self) -> Dict[str, Any]:
        """ë©”ëª¨ë¦¬ ìµœì í™”"""
        return self.cache.optimize_memory_usage()
    
    def get_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return self.cache.get_detailed_stats()
    
    def health_check(self) -> Dict[str, Any]:
        """ìºì‹œ ìƒíƒœ í™•ì¸"""
        stats = self.get_stats()
        
        health_status = {
            'status': 'healthy',
            'errors': [],
            'warnings': [],
            'details': stats
        }
        
        # íˆíŠ¸ìœ¨ í™•ì¸
        if stats['hit_rate'] < 50:
            health_status['warnings'].append(f"ë‚®ì€ ìºì‹œ íˆíŠ¸ìœ¨: {stats['hit_rate']}%")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        if stats['memory_usage_mb'] > 100:  # 100MB ì´ìƒ
            health_status['warnings'].append(f"ë†’ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage_mb']}MB")
        
        # ì‘ë‹µ ì‹œê°„ í™•ì¸
        if stats['average_response_time_ms'] > 100:  # 100ms ì´ìƒ
            health_status['warnings'].append(f"ëŠë¦° ì‘ë‹µ ì‹œê°„: {stats['average_response_time_ms']}ms")
        
        return health_status
    
    def cleanup_all_expired(self) -> Dict[str, int]:
        """ëª¨ë“  ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        try:
            # ì¼ë°˜ ìºì‹œ ì •ë¦¬
            general_cleared = self.cache.clear_old_entries()
            
            return {
                'general_cache': general_cleared,
                'total': general_cleared
            }
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                'general_cache': 0,
                'total': 0,
                'error': str(e)
            }


# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
bot_cache = BotCacheManager()


# í¸ì˜ í•¨ìˆ˜ë“¤
def get_user_daily_fortune(user_id: str, fortune_text: str = None) -> Optional[str]:
    """ìœ ì €ë³„ ì¼ì¼ ìš´ì„¸ ì¡°íšŒ/ì €ì¥"""
    return bot_cache.get_user_daily_fortune(user_id, fortune_text)


def get_fortune_phrases_cached(fetch_func: Callable) -> List[str]:
    """ìš´ì„¸ ë¬¸êµ¬ ëª©ë¡ ì¡°íšŒ (ìºì‹œ ì ìš©)"""
    return bot_cache.get_fortune_phrases(fetch_func)


def get_help_items_cached(fetch_func: Callable) -> List[Dict]:
    """ë„ì›€ë§ í•­ëª© ì¡°íšŒ (ìºì‹œ ì ìš©)"""
    return bot_cache.get_help_items(fetch_func)


def set_cache(key: str, value: Any, ttl: int = 3600) -> None:
    """ìºì‹œì— ê°’ ì €ì¥"""
    bot_cache.set(key, value, ttl)


def get_cache(key: str) -> Optional[Any]:
    """ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
    return bot_cache.get(key)


def delete_cache(key: str) -> bool:
    """ìºì‹œì—ì„œ í•­ëª© ì‚­ì œ"""
    return bot_cache.delete(key)


def clear_old_entries() -> int:
    """ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬"""
    return bot_cache.clear_old_entries()


def optimize_memory() -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ìµœì í™”"""
    return bot_cache.optimize_memory()


def get_cache_stats() -> Dict[str, Any]:
    """ìºì‹œ í†µê³„ ë°˜í™˜"""
    return bot_cache.get_stats()


def get_cache_health() -> Dict[str, Any]:
    """ìºì‹œ ìƒíƒœ í™•ì¸"""
    return bot_cache.health_check()


def clear_cache() -> None:
    """ìºì‹œ ì „ì²´ ì‚­ì œ (ë°±ì›Œë“œ í˜¸í™˜ì„±)"""
    bot_cache.cache._fortune_cache.clear()
    bot_cache.cache._content_cache.clear()
    bot_cache.cache._lru_cache.clear()
    logger.info("ìºì‹œ ì „ì²´ ì‚­ì œ ì™„ë£Œ")


def schedule_optimization():
    """
    ìºì‹œ ìµœì í™” ìŠ¤ì¼€ì¤„ë§ (ë³„ë„ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)
    """
    import threading
    import time
    
    def optimization_worker():
        """ìµœì í™” ì›Œì»¤"""
        while True:
            try:
                # 30ë¶„ë§ˆë‹¤ ìµœì í™” ì‹¤í–‰
                time.sleep(1800)
                
                # ë©”ëª¨ë¦¬ ìµœì í™”
                optimization_result = optimize_memory()
                if optimization_result['status'] == 'optimized':
                    logger.info(f"ìºì‹œ ìµœì í™” ì™„ë£Œ: {optimization_result}")
                
                # ì˜¤ë˜ëœ í•­ëª© ì •ë¦¬
                cleared = clear_old_entries()
                if cleared > 0:
                    logger.info(f"ì˜¤ë˜ëœ ìºì‹œ ì •ë¦¬: {cleared}ê°œ í•­ëª© ì œê±°")
                
            except Exception as e:
                logger.error(f"ìºì‹œ ìµœì í™” ì¤‘ ì˜¤ë¥˜: {e}")
                time.sleep(3600)  # 1ì‹œê°„ í›„ ì¬ì‹œë„
    
    # ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
    optimization_thread = threading.Thread(target=optimization_worker, daemon=True)
    optimization_thread.start()
    logger.info("ìºì‹œ ìµœì í™” ìŠ¤ì¼€ì¤„ë§ ì‹œì‘")


# ìºì‹œ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
def generate_performance_report() -> str:
    """
    ìºì‹œ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
    
    Returns:
        str: ì„±ëŠ¥ ë¦¬í¬íŠ¸
    """
    try:
        stats = get_cache_stats()
        health = get_cache_health()
        
        report_lines = ["=== ê³ ì„±ëŠ¥ ìºì‹œ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ==="]
        
        # ê¸°ë³¸ í†µê³„
        report_lines.append(f"\nğŸ“Š ìºì‹œ í†µê³„:")
        report_lines.append(f"  ìš´ì„¸ ìºì‹œ: {stats['fortune_cache_size']}ê°œ")
        report_lines.append(f"  ì½˜í…ì¸  ìºì‹œ: {stats['content_cache_size']}ê°œ")
        report_lines.append(f"  LRU ìºì‹œ: {stats['lru_cache_size']}ê°œ")
        report_lines.append(f"  ì´ ìš”ì²­: {stats['total_requests']}íšŒ")
        report_lines.append(f"  íˆíŠ¸ìœ¨: {stats['hit_rate']}%")
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­
        report_lines.append(f"\nâš¡ ì„±ëŠ¥ ë©”íŠ¸ë¦­:")
        report_lines.append(f"  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {stats['memory_usage_mb']}MB")
        report_lines.append(f"  ì••ì¶•ë¥ : {stats['compression_ratio']}")
        report_lines.append(f"  í‰ê·  ì‘ë‹µì‹œê°„: {stats['average_response_time_ms']}ms")
        report_lines.append(f"  ì••ì¶• íšŸìˆ˜: {stats['total_compressions']}íšŒ")
        report_lines.append(f"  í•´ì œ íšŸìˆ˜: {stats['total_decompressions']}íšŒ")
        
        # ìµœì í™” ì •ë³´
        report_lines.append(f"\nâœ… ìµœì í™” ì ìš©:")
        report_lines.append(f"  - LRU ìºì‹œ: ìì£¼ ì‚¬ìš©ë˜ëŠ” í•­ëª© ìš°ì„  ë³´ì¡´")
        report_lines.append(f"  - ë°ì´í„° ì••ì¶•: 1KB ì´ìƒ ìë™ ì••ì¶•")
        report_lines.append(f"  - ë©”ëª¨ë¦¬ ìµœì í™”: 30ë¶„ë§ˆë‹¤ ìë™ ì‹¤í–‰")
        report_lines.append(f"  - TTL ê´€ë¦¬: ë§Œë£Œëœ í•­ëª© ìë™ ì •ë¦¬")
        
        # ê²½ê³ ì‚¬í•­
        if health['warnings']:
            report_lines.append(f"\nâš ï¸ ê²½ê³ :")
            for warning in health['warnings']:
                report_lines.append(f"  - {warning}")
        
        return "\n".join(report_lines)
        
    except Exception as e:
        return f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}"


# ëª¨ë“ˆ ì´ˆê¸°í™” ì‹œ ìµœì í™” ìŠ¤ì¼€ì¤„ë§ ì‹œì‘
if __name__ != "__main__":
    try:
        schedule_optimization()
    except Exception as e:
        logger.warning(f"ìºì‹œ ìµœì í™” ìŠ¤ì¼€ì¤„ë§ ì‹¤íŒ¨: {e}")


def warmup_cache(sheets_manager=None):
    """
    ìºì‹œ ì›Œë°ì—… (í•„ìˆ˜ ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ)
    
    Args:
        sheets_manager: Google Sheets ë§¤ë‹ˆì € (ì„ íƒì‚¬í•­)
    """
    try:
        logger.info("ğŸ”¥ ìºì‹œ ì›Œë°ì—… ì‹œì‘...")
        
        # ê¸°ë³¸ ìºì‹œ ì´ˆê¸°í™”
        bot_cache.set("warmup_status", "completed", 3600)
        
        # Google Sheetsê°€ ìˆëŠ” ê²½ìš° ìš´ì„¸ ë¬¸êµ¬ ë¯¸ë¦¬ ë¡œë“œ
        if sheets_manager:
            try:
                # ìš´ì„¸ ë¬¸êµ¬ ë¯¸ë¦¬ ë¡œë“œ
                def fetch_fortune_phrases():
                    return sheets_manager.get_fortune_phrases()
                
                fortune_phrases = bot_cache.get_fortune_phrases(fetch_fortune_phrases)
                if fortune_phrases:
                    logger.info(f"âœ… ìš´ì„¸ ë¬¸êµ¬ {len(fortune_phrases)}ê°œ ë¯¸ë¦¬ ë¡œë“œ ì™„ë£Œ")
                
                # ë„ì›€ë§ í•­ëª© ë¯¸ë¦¬ ë¡œë“œ
                def fetch_help_items():
                    return sheets_manager.get_help_items()
                
                help_items = bot_cache.get_help_items(fetch_help_items)
                if help_items:
                    logger.info(f"âœ… ë„ì›€ë§ í•­ëª© {len(help_items)}ê°œ ë¯¸ë¦¬ ë¡œë“œ ì™„ë£Œ")
                    
            except Exception as e:
                logger.warning(f"âš ï¸ Google Sheets ë°ì´í„° ë¯¸ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ í†µê³„ ì´ˆê¸°í™”
        bot_cache.set("cache_stats", {
            "initialized_at": time.time(),
            "version": "1.0"
        }, 86400)  # 24ì‹œê°„
        
        logger.info("âœ… ìºì‹œ ì›Œë°ì—… ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì›Œë°ì—… ì‹¤íŒ¨: {e}")
        raise


def start_cache_cleanup_scheduler(interval: int = 300):
    """
    ìºì‹œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘
    
    Args:
        interval: ì •ë¦¬ ê°„ê²© (ì´ˆ, ê¸°ë³¸ê°’: 300ì´ˆ = 5ë¶„)
    """
    try:
        logger.info(f"ğŸ”„ ìºì‹œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ (ê°„ê²©: {interval}ì´ˆ)")
        
        def cleanup_worker():
            """ì •ë¦¬ ì›Œì»¤"""
            while True:
                try:
                    time.sleep(interval)
                    
                    # ì˜¤ë˜ëœ í•­ëª© ì •ë¦¬
                    cleared_count = clear_old_entries()
                    if cleared_count > 0:
                        logger.info(f"ğŸ§¹ ìºì‹œ ì •ë¦¬: {cleared_count}ê°œ í•­ëª© ì œê±°")
                    
                    # ë©”ëª¨ë¦¬ ìµœì í™” (30ë¶„ë§ˆë‹¤)
                    if int(time.time()) % 1800 < interval:
                        optimization_result = optimize_memory()
                        if optimization_result['status'] == 'optimized':
                            logger.info(f"âš¡ ë©”ëª¨ë¦¬ ìµœì í™” ì™„ë£Œ: {optimization_result.get('memory_saved_mb', 0):.2f}MB ì ˆì•½")
                    
                except Exception as e:
                    logger.error(f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    time.sleep(60)  # 1ë¶„ í›„ ì¬ì‹œë„
        
        # ë°ëª¬ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰
        cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
        cleanup_thread.start()
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ì €ì¥
        bot_cache.set("cleanup_scheduler", {
            "started_at": time.time(),
            "interval": interval,
            "status": "running"
        }, 86400)
        
        logger.info("âœ… ìºì‹œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘ ì‹¤íŒ¨: {e}")
        raise


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_optimized_cache():
    """ìµœì í™”ëœ ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("=== ê³ ì„±ëŠ¥ ìºì‹œ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ===")
    
    # ìš´ì„¸ ìºì‹œ í…ŒìŠ¤íŠ¸
    test_user = "test_user"
    test_fortune = "ì˜¤ëŠ˜ì€ ì¢‹ì€ ì¼ì´ ìˆì„ ê²ƒì…ë‹ˆë‹¤."
    
    # ì €ì¥
    result1 = get_user_daily_fortune(test_user, test_fortune)
    print(f"ìš´ì„¸ ì €ì¥: {result1}")
    
    # ì¡°íšŒ
    result2 = get_user_daily_fortune(test_user)
    print(f"ìš´ì„¸ ì¡°íšŒ: {result2}")
    
    # ì¼ë°˜ ìºì‹œ í…ŒìŠ¤íŠ¸
    set_cache("test_key", "test_value", 60)
    cached_value = get_cache("test_key")
    print(f"ì¼ë°˜ ìºì‹œ: {cached_value}")
    
    # í†µê³„ í™•ì¸
    stats = get_cache_stats()
    print(f"ìºì‹œ í†µê³„: {stats}")
    
    # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±
    report = generate_performance_report()
    print(f"\n{report}")
    
    print("=" * 40)


if __name__ == "__main__":
    test_optimized_cache()